{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  artist                   song  \\\n",
      "0   ABBA  Ahe's My Kind Of Girl   \n",
      "1   ABBA       Andante, Andante   \n",
      "2   ABBA         As Good As New   \n",
      "3   ABBA                   Bang   \n",
      "4   ABBA       Bang-A-Boomerang   \n",
      "\n",
      "                                                text  \n",
      "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
      "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
      "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
      "3  Making somebody happy is a question of give an...  \n",
      "4  Making somebody happy is a question of give an...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('../data/spotify_millsongdata.csv')\n",
    "\n",
    "# Drop the 'link' column as it's not needed for the recommender\n",
    "data.drop('link', axis=1, inplace=True)\n",
    "\n",
    "# Display the first few rows to understand what the data looks like\n",
    "print(data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 3)\n",
      "         artist                       song  \\\n",
      "0  Wishbone Ash             Right Or Wrong   \n",
      "1     Aerosmith  This Little Light Of Mine   \n",
      "2  Fall Out Boy               Dance, Dance   \n",
      "3  Janis Joplin                 Easy Rider   \n",
      "4   Moody Blues                  Peak Hour   \n",
      "\n",
      "                                                text  \n",
      "0  Like to have you 'round  \\r\\nWith all the lies...  \n",
      "1  This Little Light of Mine (Light of Mine),  \\r...  \n",
      "2  She says she's no good with words but I'm wors...  \n",
      "3  Hey mama, mama, come a look at sister,  \\r\\nSh...  \n",
      "4  I see it all through my window it seems.  \\r\\n...  \n"
     ]
    }
   ],
   "source": [
    "# Sample 10,000 records from the dataset\n",
    "data_sampled = data.sample(n=10000, random_state=42)  # Seed for reproducibility\n",
    "\n",
    "# Reset index after sampling\n",
    "data_sampled.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the shape and head of the sampled data to verify\n",
    "print(data_sampled.shape)\n",
    "print(data_sampled.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    like round lie make thing darkness people say ...\n",
      "1    little light mine light mine im let shine alei...\n",
      "2    say shes good word im worse barely stuttered j...\n",
      "3    hey mama mama come look sister shes astanding ...\n",
      "4    see window seems never failing like million ee...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download stopwords and wordnet data\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize text\n",
    "    words = text.split()\n",
    "    \n",
    "    # Remove stopwords and lemmatize\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stopwords.words('english')]\n",
    "    \n",
    "    # Join words to form the cleaned up text\n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to each entry in the 'text' column\n",
    "data_sampled['text'] = data_sampled['text'].apply(preprocess_text)\n",
    "\n",
    "# Verify preprocessing\n",
    "print(data_sampled['text'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 33481)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed 'text' column\n",
    "tfidf_matrix = tfidf.fit_transform(data_sampled['text'])\n",
    "\n",
    "# Check the shape of the resulting TF-IDF matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.04397211 ... 0.02830541 0.03655577 0.0710031 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Check the first element to verify calculations\n",
    "print(cosine_sim[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9226                     Age\n",
      "2373             i hate boys\n",
      "4259        Along Came Jones\n",
      "2382          Living In Fame\n",
      "3612          Hammer To Fall\n",
      "6247         Light The Shade\n",
      "2820    Sweeter Than Fiction\n",
      "6822    My Baby's Good To Me\n",
      "8744            Bold As Love\n",
      "4850          Le Ballet D'or\n",
      "Name: song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def recommend_songs(song_title, data, cosine_sim, top_n=10):\n",
    "    # Check if the song is in the dataset\n",
    "    if song_title not in data['song'].values:\n",
    "        return f\"No recommendations found: '{song_title}' is not in the dataset.\"\n",
    "\n",
    "    # Find the index of the song that matches the title\n",
    "    idx = data[data['song'] == song_title].index[0]\n",
    "\n",
    "    # Get the pairwise similarity scores of all songs with that song\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # Sort the songs based on the similarity scores in descending order\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Get the scores of the top-n most similar songs\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "\n",
    "    # Get the song indices\n",
    "    song_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # Return the top-n most similar songs\n",
    "    return data['song'].iloc[song_indices]\n",
    "\n",
    "# Make sure to replace 'Enter Your Song Title Here' with an actual song title from your dataset\n",
    "test_song_title = 'Salute'\n",
    "print(recommend_songs(test_song_title, data_sampled, cosine_sim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1817      Riot Reunion\n",
      "6115          Case 795\n",
      "7911           Forever\n",
      "3400      Messiah Ward\n",
      "7705    Wild And Crazy\n",
      "Name: song, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data_sampled['song'].sample(5))  # Print five random song titles from the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(cosine_sim,open('similarity.pkl','wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(data,open('df.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment2-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
